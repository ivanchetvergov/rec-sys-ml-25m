1)
смотри вот структура датасета
[2 JSON файла отвечающие за полноту и метаданные датасетов]

наша задача сейчас написать препроцессинг.

1. считываем данные из разных таблиц
2. соединияем все в одно полотно
3. делаем feature engenireeng
4. храним все данные в общем feature store

оформляем все эти вещи как самостоятельные классы пишем краткий понятный и поддерживаемый код и начинаем вести makefile для автоматизации
не перегружай его излишним ф0оналом добавь туда просто этап препроцессинга
и пайтон я использую глобальный из user/ivan/myvenv

2) давай реализуем модель popularity-based с мониторингом ml-flow, которая будет читать лишь часть данных вместо всего хранилища для более простого тестирования и работы модели

3) давай начнем с малого, давай реализуем базовую страницу главную где будут наши самые популярные фильмы

пиши код как и ранее в виде классов по всем принципам, самое главное не усложняй код, там где это не нужно

4) напиши несколько документаций к каждому слою программы, должна быть общая директория docs/ там архитектура, веб с ручками, бекенд, документ по API и тд

5) давай поработаем с форматом страницы в вебе, хочу получить результат схожий с netflix

я хочу сделать страницу в несколько слоев

1. слой примерно центр экрана в котором будет самые популярные
2. персональные рекомендации, пока оставим его пустым или давай случайные туда подсуним
3. так же сделаем заготовку под каталог фильмов

пока не реализуй глубокий функционал, поработай над страницами и их стилями

6) давай реализуем теперь карточки для каждого фильма
что в них должно быть и реализуем загрузку постеров по внешнему API, вроде такой ф-ионал позволяет реализовать links.csv:

1. постер
2. описание
3. рейтинг
4. поле для отзыва
5. поле для оценки
